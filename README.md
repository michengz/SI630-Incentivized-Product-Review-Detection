# SI650-Incentivized-Product-Review-Detection
NLP Course Final Project @ University of Michigan
## Overview
As people have increasingly relied on online product reviews to make purchasing decisions nowadays, the problem of companies using incentivized reviews to boost brand exposures have continued to rise, especially in the beauty product industry. For this project, we will be performing text classification to identify incentivized product reviews from Sephora by using neural network models such as BERT and LSTM, as well as other fundamental linear models like Naive Bayes, SVM, and Logistic Regression.
## Data Collection 
One of the beauty product websites that is well known for having mass amounts of incentivized reviews is Sephora, which led to continuous complaints and discussions from customers over the years. To tackle this problem, the Sephora website recently introduced new features such as marking reviews as “Incentivized” or as “Verified Purchase” when specific standards are met. Given this feature, we are able to utilize these tags as our labels for categorizing the reviews. We scraped data from the Sephora website using BeautifulSoup, and obatined reviews of over 1,000 skincare products with 184,638 reviews in total. Up to 101,261 reviews in this dataset were neither marked as incentivized nor marked as verified purchases on the Sephora website. In order to accurately classify reviews as incentivized and non-incentivized, only reviews that are marked incentivized and non-verified purchases were considered as “Incentivized Reviews”. On the other hand, reviews that are marked as verified purchases and have no incentivized tags were considered as “Non-incentivized Reviews”. As such, our dataset used for actual training only consisted of 66,032 (79.5 %) incentivized reviews and 16,928 (20.5 %) non-incentivized reviews
## Methods
For this project, both statistical linear models and deep neural networks are experimented. To effectively compare results between these two sorts of models, we will improve performance when using linear classifiers by using feature extraction techniques such as TD-IDFs with n-grams. Prior to training each of our models, all of them follow the general framework of going through text preprocessing, data transformation, and model building. For text preprocessing, the review title and review text of reviews will first be concatenated prior to tokenization. Next, all documents will be passed into the tokenizer that tokenizes the inputs, removes stopwords, removes punctuations, and lemmatizes words to its original form. The steps for tokenization, stopword removal, and lemmatization were all completed by using the NLTK library. All models that I have created share the same tokenization methods in terms of text preprocessing. 
For data transformation, all datasets for the models were processed with a 80/20 train-test split. After that, ways for generating training datasets and performing feature extraction techniques start to differ across models. We will break into the following three parts, Statistical Linear Models, Bi-LSTM and BERT to explain how their datasets were generated, as well as how and why the models were constructed.
### 1. Statistical Linear Models
This project started off by training fundamental linear models such as Naive Bayes, SVM, and Logistic Regression that are common for solving text classification tasks. To improve the performance of these models, we experimented with using unigram-based TF-IDFs and both unigram and bigram-based TF-IDFs for feature extraction. This was done by using the TfidfVectorizer from Sklearn, where it takes in arguments such as the tokenizer used and the n-gram range for fit-transforming the training data. The transformed datasets are then passed into the three different classifiers by using the Sklearn library, where the MultinomialNB classifier was used for Naive Bayes, the SGDClassifier used for Linear SVM, and the LogisticRegression classifier used for logistic regression.
### 2. Biderectional LSTM
The first neural network model that we created is the LSTM model, a type of Recurrent Neural Network(RNN) that is better than the traditional RNNs since it has the advantage of memorizing information that are relatively important. Different from the linear models from the previous section, the LSTM algorithm would allow meaning within documents to be captured and represented. We also chose to build a bidirectional LSTM model since it takes information in both directions and could train the model with a larger dimension. Prior to building the model, the training dataset was generated using the Torchtext library from Pytorch. This helps build our vocabularies for word embeddings based on our training documents. The inputs are also passed in with batches created by using the BucketIterator from Torchtext. Next, the LSTM model was built with four layers using Pytorch. The first layer is the word embedding layer, where it creates word embeddings with the size of vocabulary size by the hidden dimension (100), and then packs the padded sequences into batches. The packed outputs are then passed into the second layer, which is the LSTM layer. Since our LSTM is set as bidirectional, the last hidden states of the two directions concatenated at this layer as well. The outputs of the hidden states are then passed into a linear layer to produce dense outputs. Lastly, since we are dealing with a binary classification task, the sense outputs are passed into a sigmoid activation layer to output the final probabilities.
### 3. BERT MiniLM
Though LSTMs are good at dealing with the problem of vanishing gradients, the BERT model is relatively new and well known for being a better way to capture meaning within a sentence because of its Masked Language Model approach. To build the BERT model, we used the pre-trained MiniLM model provided by Microsoft Huggingface. The dataset features with the input ids and the attention masks are first generated using the Dataset library. We then extended the Trainer class and created a WeightedLossTrainer that fixes the imbalanced dataset through calculating loss functions with class weights. The transformed datasets are then passed into the trainer for training and evaluation.
